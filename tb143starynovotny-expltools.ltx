\documentclass[final]{ltugboat}
\usepackage[T1]{fontenc} % tt \_
\usepackage{lmodern}
\usepackage{hologo}
\usepackage{microtype}
\usepackage{soul}
\usepackage{transparent}
\usepackage{verbatimbox}
\usepackage{enumitem}
\usepackage[font=small, labelfont=bf, justification=justified, format=plain]{caption}
\usepackage{minted}
\setminted{breaklines}
\usemintedstyle{bw}
\def\slash{\discretionary{/}{/}{/}}
\def\hyphen{\discretionary{-}{-}{-}}
\DeclareUnicodeCharacter{2014}{\Dash}
\def\verb{\mintinline{text}}
\usepackage{tikz}
\usetikzlibrary{
  arrows.meta,
  decorations.pathmorphing,
  decorations.pathreplacing,
  positioning,
  tikzmark,
}

\usepackage[hidelinks,pdfa]{hyperref}
\def\url{\tbsurl}
% silence font warnings:
% LaTeX Font Warning: Font shape `OT1/cmr/m/n' in size <45> not available
% (Font)              size <24.88> substituted on input line 94.
\makeatletter\def\@font@warning#1{}\makeatother

%%% Start of metadata %%%
\title{Expltools: Development tools for expl3 programmers}

% repeat info for each author; comment out items that don't apply.
\author{V\'{i}t Star\'{y} Novotn\'{y}}
\address{Studen\'{a} 453/15 \\ Brno 63800, Czech Republic}
\netaddress{witiko (at) mail dot muni dot cz}
\personalURL{github.com/witiko}

\author{Oliver Kopp}
\address{Sindelfingen, Germany}
\ORCID{0000-0001-6962-4290}

%%% End of metadata %%%

% Note to editor: If possible, please arrange for the first page to fall on an even-numbered page, so that:
%
% - The introduction and outline appear side by side.
% - Figure 2 and sections 4.2 through 4.4 appear side by side.
% - Figure 3 and Sections 5.1 appear side by side.
% - Figure 4 and Sections 5.2 appear side by side.
% - Figure 5 and Sections 5.3 appear side by side.
%
% That said, no worries if it doesn't work out\Dash I'm sure the article will still look great, thanks in no small part to your careful editing.

\begin{document}
\maketitle

\begin{abstract}
Static analysis tools are invaluable for catching bugs early and improving code quality. However, the dynamic nature of \TeX{} has long posed a barrier to static analysis.

Expl3 is a high-level programming layer built on top of \TeX. Originally developed to simplify the \LaTeX{} kernel, it is now widely used by package authors across \LaTeX{} and other \TeX{} formats. While it retains much of \TeX's flexibility, real-world expl3 code tends to follow more predictable patterns, making it far more amenable to static analysis.

In this article, we introduce \emph{expltools}\Dash a long-overdue addition to the expl3 programmer's toolbox. Designed to support semantic analysis, it brings clarity and structure to a domain traditionally resistant to static tooling. We present the current state of the project, focusing on its design, practical usage, and validation on real-world expl3 packages.
\end{abstract}

\section{Introduction}
Most modern programming languages\Dash whether interpreted or compiled\Dash share a broadly similar syntax, differing mainly in the specific keywords and symbols used to express language constructs. Even interpreted languages such as Lua, Python, and JavaScript often inherit their syntactic conventions from compiled languages like Pascal and C. This inheritance persists partly due to familiarity and momentum, but also because predictable syntax offers significant advantages for code performance and tooling.

\subsection{Why predictable syntax matters}
% TODO: Add an index at the end of the article for all terms defined via \emph and \acro.
One key advantage of predictable syntax is that it enables \emph{static analysis}\Dash automated reasoning about code without running it. Static analysis underpins many tools that help developers write, understand, and improve their code.

Text editors, for example, can offer smarter features thanks to predictable syntax. Even simple syntax highlighting\Dash using different colors for keywords, variables, and comments\Dash makes the code structure easier to read. Going further, editors can index the names and types of variables, functions, and other objects, enabling smarter code completion, more accurate navigation, and robust search across a project. With deeper static analysis, they can even support complex refactoring\Dash such as renaming a variable consistently across its entire scope.

Language interpreters also benefit: they can often detect and report syntax errors before executing the program, simply by parsing the code and enforcing grammatical rules.

\subsection{Letting linters do the boring work}
In addition to editors and interpreters, many languages have tools called \emph{linters}, which use techniques from compiler design to catch common mistakes. Linters can flag undefined variables, wrong argument counts, or type mismatches\Dash for example, passing a number where a string is expected.

Linters complement human-written tests: while tests verify dynamic behavior at runtime, they must be explicitly written to cover each relevant case. Linters, on the other hand, scan through all your code up front. They might not catch everything, and their checks are usually more general, but they can flag a lot of potential issues right away\Dash before you've written a single test or even run the program.

% TODO: Clarify (in a footnote?) that, in the following two sections, when we speak about "TeX", we mean low-level TeX code that consists primarily of TeX primitives and potentially also plain TeX macros.
\subsection{What makes \TeX{} hard to analyze}
Unlike languages like Lua, Python, or JavaScript, which come from the tradition of numerical and procedural programming, \TeX{} descends from a different lineage: \emph{macro processors}, where programs are built by defining and expanding text-based substitutions (so-called \emph{macros}). This heritage introduces unique challenges for static analysis.

In macro languages, program structure often isn't visible in the source code\Dash it only emerges after macro expansion. \TeX{} goes even further: it allows the meaning of individual characters to change at runtime, further obfuscating the surface syntax.

On top of that, macro languages like \TeX{} lack type annotations, function signatures, and other cues that clarify programmer intent. Without these, it's hard to distinguish variables from functions, since both are represented by macros. This makes even basic checks\Dash such as validating argument counts or data types\Dash difficult to perform.

Scoping adds to the complexity. Most modern languages use \emph{lexical scoping}, where the meaning of an identifier is determined by its position in the code. \TeX{}, however, uses \emph{dynamic scoping}, where the meaning of a macro depends not on where it appears, but on the state of the execution environment when it is used. This makes it hard for tools to reason about the flow of control and data in \TeX{} programs.

All of this makes \TeX{} inherently resistant to static analysis. Even simple editor features like syntax highlighting can be unreliable. While tools can handle constrained subsets of the language, analyzing arbitrary \TeX{} code remains fundamentally limited.

\subsection{How expl3 tames the chaos}
\emph{Expl3}~\cite{latex2025expl3, peischl2023introduction} is a high-level programming language built on top of \TeX{}, designed to simplify the internal implementation of the \LaTeX{} format and serve as a programming layer for \LaTeX{} package authors. In practice, it can also be used with other \TeX{} formats, such as plain \TeX{} and \Hologo{ConTeXt}, making it a viable replacement for \TeX{} in many contexts. Compared to \TeX{}, expl3 introduces a number of conventions that make analysis far more tractable.

Although expl3 still permits changing the meaning of individual characters at runtime, such behavior is rarely needed, thanks to higher-level abstractions and built-in data types. This improves the consistency of surface syntax.

Unlike \TeX{}, expl3 clearly distinguishes variables from functions. Variable names include metadata such as type, scope, and mutability, while function names specify argument counts, types, and even visibility (public vs.\ private). This kind of structured interface makes it possible to perform checks\Dash like validating argument counts or detecting type mismatches\Dash that are nearly impossible in \TeX{}.

Scoping remains dynamic in expl3, but the language encourages the use of function arguments over unbound variables. In particular, function variants that take \texttt{V}- or \texttt{v}-type arguments~\cite[Section~1.1]{latex2025interface3} promote a pass-by-value style, reducing unintended interactions through dynamic scope.

While these conventions are not enforced by the runtime, many expl3 packages adopt them, making the syntax more predictable and amenable to static analysis. Despite this, there was no editor support, linter, or other static tooling available for expl3 as recently as a year ago.

\subsection{Toward better tooling for expl3}
Between April and August 2024, we published a series of blog posts on the static analysis of expl3~\cite{starynovotny2024statica, starynovotny2024staticb, starynovotny2024staticc, starynovotny2024staticd}, culminating in a proposal for a year-long project to develop \emph{explcheck}\Dash an expl3 linter\Dash submitted to the \TeX{} Development Fund in September~\cite{starynovotny2024project}. The proposal was accepted, and over the following two months, we worked toward the initial release.

\begin{figure}[t]
\centering
\input images/platypus-detective-seabed
\caption{Explcheck's mascot is \emph{Platy}, an underwater detective who dives beneath the surface of your expl3 code to uncover hidden issues.\\[1ex] The concept was suggested by Paulo Cereda, and the illustration was created by Greg at \tbsurl{https://quickcartoons.com}. All illustrations of Platy are licensed under \acro{CC-BY}, so you're free to use them in your own promotional materials and derivative works, provided you credit the artist~\cite{starynovotny2025platy}.}
\end{figure}

On December 5, we demonstrated a pre-release version to Frank Mittelbach during their visit to Brno~\cite{starynovotny2024statice}. The first public release (v0.1.0) followed on December 14~\cite{starynovotny2024release}, and was presented at the general assembly of the Czech and Slovak \TeX{} Users Group (\CSTUG) on December 16~\cite{starynovotny2024staticf}.

% TODO: change to "reaching a stable v1.0.0 in XXX" after the v1.0.0 release.
Since then, we have continued development with monthly updates, aiming for a stable v1.0.0. Progress has been regularly documented on the first author's blog~\cite{starynovotny2025expl3}, as well as on Twitter/X~\cite{starynovotny2024tweet} and Mastodon~\cite{starynovotny2024toot}.

Currently, explcheck is the first and only tool in what will become the \emph{expltools} bundle~\cite{starynovotny2025ctan}. In the future, we plan to extend expltools into a suite of static analysis tools for expl3, all built on a common library of reusable components. Explcheck serves as the foundation for this broader toolkit.

\section{Outline}
In this article, we present the current state of explcheck, emphasizing its design, demonstrating its practical use, and validating its results on real code.

Section~\ref{sec:requirements} begins with a requirements analysis, covering both the intended scope of explcheck's functionality and other considerations. This section closely follows our original project proposal, and can be skipped if you're not interested in the process.

Sections~\ref{sec:design} (p.~\pageref{sec:design}) and \ref{sec:implementation} outline explcheck's high-level design and implementation. Section~\ref{sec:demonstration} demonstrates its use, and Section~\ref{sec:validation} validates its results across all expl3 packages in \TeX{} Live (2013--2025) via continuous analysis and a set of experiments developed for this article.

In Section~\ref{sec:future-work}, we discuss current limitations and suggest directions for future development. Section~\ref{sec:related-work} surveys related efforts in static analysis of \TeX{} and other programming languages more broadly, highlighting influences on explcheck's design. Finally, Section~\ref{sec:conclusion} summarizes our contributions.

\section{Requirements}
\label{sec:requirements}
In this section, we outline our goals for explcheck. Section~\ref{sec:functional-requirements} identifies the core features the tool should provide, while Section~\ref{sec:non-functional-requirements} covers broader considerations such as the classification of reported issues, software architecture, validation, and licensing.

\subsection{Functional requirements}
\label{sec:functional-requirements}
Explcheck should accept a list of input expl3 files, analyze each one, and report any issues it identifies.

In its initial version, the linter should detect at least the following types of issues, which we have grouped into three broad categories:
\medskip

\paragraph{Style issues:}
\begin{itemize}
\item Overly long lines
\item Missing stylistic whitespace
\item Malformed names of functions, variables, constants, quarks, or scan marks
\end{itemize}

\paragraph{Function issues:}
\begin{itemize}
\item Multiply defined functions and function variants
\item Calls to undefined functions and variants
\item Calls to deprecated or removed\footnote{Deprecated and removed expl3 material is listed in the file \href{https://github.com/latex3/latex3/blob/main/l3kernel/doc/l3obsolete.txt}{\texttt{l3obsolete.txt}}, which is maintained by the \LaTeX{} team.} functions
\item Unknown argument specifiers
\item Unexpected arguments in function calls
\item Unused private functions and variants
\end{itemize}

\paragraph{Variable and constant issues:}
\begin{itemize}
\item Multiply declared variables and constants
\item Using undefined variables or constants
\item Using variables with incompatible types
\item {\raggedright Using deprecated or removed variables and constants\par}
\item Setting constants or undeclared variables
\item Unused variables and constants
\item Locally setting global variables (and vice versa)
\end{itemize}
\medskip

\subsection{Non-functional requirements}
\label{sec:non-functional-requirements}
In this section, we outline several non-functional aspects of explcheck. Section~\ref{sec:issues} defines the two fundamental classes of issues the tool should report, and describes how individual issues should be identified and handled by the linter. Section~\ref{sec:architecture} addresses the choice of programming language and key architectural considerations. Section~\ref{sec:requirements-validation} discusses how explcheck should be validated during development. Finally, Section~\ref{sec:license-terms} specifies the licensing terms.

\subsubsection{Issues}
\label{sec:issues}
The linter should distinguish between two classes of issues: \emph{warnings} and \emph{errors}. As a rule of thumb, warnings highlight violations of best practices, while errors correspond to issues likely to cause incorrect behavior at runtime.
\medskip

\paragraph{Examples of warnings:}
\begin{itemize}
\item Missing stylistic whitespace
\item Using deprecated functions or variables
\item Unused variable or constant
\end{itemize}

\paragraph{Examples of errors:}
\begin{itemize}
\item Using an undefined message
\item Calling a function with a \texttt{V}-type argument on a variable that does not support \texttt{V}-type expansion
\item Multiple declarations of one variable/constant
\end{itemize}
\medskip

The overriding design goal for early releases of the linter should be implementation simplicity and robustness to unexpected input. When reporting issues, the linter should only emit a warning or error when it is reasonably confident that the code has been correctly understood\Dash even if that means some issues may go undetected.

Each issue should be assigned a unique identifier. These identifiers allow flexible suppression of specific issues: globally via a configuration file, per input file from the command line, or locally in the source code using \TeX{} comments.

\subsubsection{Architecture}
\label{sec:architecture}

To make the linter easy to run in continuous integration pipelines, it should be written in Lua~5.3, as this is the version used by recent releases of Lua\TeX{}, and should rely only on the standard Lua library and the external library \acro{LPEG}~\cite{ierusalimschy2009text, menke2019parsing} for pattern-matching.

The linter should process input files in a series of well-defined steps, each implemented as a separate Lua module. These modules should be usable on their own, so users can drop them into other Lua code without having to rely on the rest of the linter.

Each step should take input from the previous step, look for issues, and convert the data into a format the next step can use. The default command-line script should run all steps in order and print any issues it finds. Users should be able to tweak this script easily\Dash for example, to:

\begin{enumerate}
\item change how input files are discovered,
\item add, remove, or rearrange processing steps, and
\item change how the linter reacts to the issues it finds.
\end{enumerate}

The linter should also work well with text editors. Ideally, it should either support the Language Server Protocol (\acro{LSP}), or be designed so that writing an \acro{LSP} wrapper is straightforward.

\subsubsection{Validation}
\label{sec:requirements-validation}
As part of a test-driven development paradigm, each type of issue detected by a processing step should have at least one corresponding test in the linter's code repository~\cite{starynovotny2025github}. These tests should be run regularly throughout development to ensure the correctness and stability of the tool.

As part of what we might only half-jokingly call a dogfooding paradigm, the linter should be used in the continuous integration pipeline of the first author's Markdown package for \TeX. This will help uncover practical issues and gather early feedback in a real-world setting. Other brave early adopters are warmly encouraged to try it out and report any bugs or oddities in the code repository.

At a later stage, a broader validation should be conducted as part of a TUGboat article presenting the linter (yes, this article) to the wider \TeX{} community. This validation should involve running the linter on all expl3 packages from both current and historical \TeX{} Live distributions. The results should be evaluated both quantitatively and qualitatively: the quantitative analysis should focus on trends in how expl3 is used in practice, while the qualitative analysis should highlight the linter's limitations and suggest directions for improvement.

\subsubsection{License terms}
\label{sec:license-terms}
The linter should be free software and dual-licensed under the following two software licenses:
\begin{enumerate}
\item \acro{GNU} General Public License (\acro{GNU GPL})${}\geq{}$2.0
\item \LaTeX{} Project Public License (\acro{LPPL})${}\geq{}$1.3c
\end{enumerate}

The option to use \acro{GNU GPL}~2.0 or later is motivated by the fact that \acro{GNU GPL}~2.0 and~3.0 are mutually incompatible. Supporting both \acro{GNU GPL}~2.0 and~3.0 extends the number of free open-source projects that will be able to alter and redistribute the linter.

The option to use \acro{LPPL}~1.3c is motivated by the fact that it imposes very few licensing restrictions on \TeX\ users. Furthermore, it also preserves the integrity of \TeX\ distributions by enforcing its naming and maintenance clauses, which ensure ongoing project stewardship and prevent confusion between modified and official versions.

Admittedly, \acro{GNU GPL} and \acro{LPPL} may seem like an unusual combination, since \acro{GNU GPL} is a copyleft license whereas \acro{LPPL} is a permissive license. However, there are strategic benefits to offering both.

We would offer \acro{LPPL} as the primary license for derivative works within the \TeX\ ecosystem. One downside of using \acro{LPPL} is that it could potentially allow bad actors to create proprietary derivative works without contributing back to the original project. However, this trade-off helps maintain the \TeX\ ecosystem's consistency and reliability. Incidentally, there is an element of trust in the \TeX\ user community to voluntarily contribute improvements back, even though the license itself does not mandate it.

We would offer \acro{GNU GPL} as an alternative license for derivative works outside the \TeX\ ecosystem. The key benefit of including \acro{GNU GPL} is that it enables the code to be integrated into free open-source projects, especially those with licenses that are incompatible with \acro{LPPL}'s naming requirements. This opens the door for broader collaboration with the free software community.

Notably, \acro{GNU GPL} creates a one-way licensing situation: once a derivative work is licensed under \acro{GNU GPL}, it cannot be legally re-licensed under a less restrictive license like \acro{LPPL}. As a result, we wouldn't be able to incorporate changes made to \acro{GNU GPL}-licensed works back into the original project under \acro{LPPL} without also creating two forks of the project licensed under \acro{GNU GPL}~2.0 and \acro{GNU GPL}~3.0, respectively. While this might seem like a downside, we view it as an important counterbalance to the potential for proprietary derivative works under \acro{LPPL}.

In summary, this dual-licensing approach allows us to maintain the integrity of the \TeX\ ecosystem while making the project more accessible to the broader free open-source community. It provides flexibility for different use cases, though we will need to carefully manage contributions to ensure compliance with all licenses.

\section{Design}
\label{sec:design}
In this section, we outline the high-level design of explcheck. Section~\ref{sec:processing-steps} describes how input files are processed. Section~\ref{sec:warnings-and-errors} discusses how issues are named and organized. Section~\ref{sec:confidence-based-processing} discusses how the linter handles uncertainty. Finally, Section~\ref{sec:limitations} describes the fundamental limitations of the current design.

\subsection{Processing steps}
\label{sec:processing-steps}

As outlined in Section~\ref{sec:functional-requirements}, the linter processes input files in a series of discrete \emph{processing steps}, each implemented as a separate Lua module:

\begin{enumerate}
\item \textbf{Preprocessing} segments input files into \emph{parts} that contain expl3 code.
\item \textbf{Lexical analysis} reads the parts as \emph{\TeX{} tokens}.
\item \textbf{Syntactic analysis} merges tokens into \emph{calls}.
\item \textbf{Semantic analysis} classifies calls as \emph{statements}.
\item \textbf{Flow analysis} merges statements into \emph{chunks} of well-understood code organized in a \emph{flow graph}.
\end{enumerate}

Figure~\ref{fig:iceberg} illustrates the full pipeline applied to an example \LaTeX{} document with expl3 code.
\looseness=-1

\begin{figure*}[p]
\centering
% TODO: Check that the issues from the figure are correctly reported by explcheck v1.0.0.
\input images/iceberg
\caption{Processing an example \LaTeX{} document \texttt{example.tex} (top left) with Lua\LaTeX{} (top right) and explcheck (bottom). While compiling with Lua\LaTeX{} produces a \acro{PDF} without warnings or errors, running the same code through explcheck allows us to dive beneath the surface and uncover many issues worth fixing. Illustration by Gabriela Fišerová.\looseness=-1}
\label{fig:iceberg}
\end{figure*}

\subsection{Warnings and errors}
\label{sec:warnings-and-errors}

Each processing step builds upon the results of its predecessor and produces data structures that reflect the current level of analysis. While a step is running, it records the issues pertinent to that level, and once the step completes, later steps focus on the data it produced. Thus almost every issue is most naturally reported at the step where the required information first becomes available, creating a clear parent–child relationship between processing steps and issues.

An issue cannot be detected until the program has been analyzed deeply enough. For example, the stream opened on line 4 of the document in Figure~\ref{fig:iceberg} is visible to both syntactic and semantic analysis, but only the flow analysis step has the flow graph needed to determine whether every execution path eventually closes the stream. Potential resource leaks are therefore detected exclusively by flow analysis.

Conversely, some issues become invisible after a certain point. For example, whitespace is stylistic and produces no \TeX{} tokens in expl3. Therefore, the missing whitespace on lines 6 and 12 can only be reported by lexical analysis, because all following steps operate on \TeX{} tokens and derived data structures.
\medskip

Based on the requirements from sections~\ref{sec:functional-requirements} and \ref{sec:issues}, and on the relationship between issues and processing steps, we drafted an initial list of 26 warnings and 40 errors for the project proposal~\cite{starynovotny2024warnings}.

These issues are organised into five sections\Dash one per processing step\Dash and each is paired with a name, an identifier, a short description, and one or more code examples that serve both as documentation and as test files. The identifier begins with \texttt{W} (general) or \texttt{S} (style) for warnings, and \texttt{E} (general) or \texttt{T} (type) for errors; this is followed by a digit between \texttt{1} and \texttt{5} indicating the processing step, and finally a two-digit serial number for the individual issue.

For example, the issues visible in Figure~\ref{fig:iceberg} are:

\begin{itemize}
  \item \texttt{W101} (Unexpected delimiters)
  \item \texttt{S204} (Missing stylistic whitespace)
  \item \texttt{W302} (Unbraced \texttt{n}-type function call argument)
  \item \texttt{W402} (Unused private function variant)
  % TODO: Verify the identifier W522 once semantic analysis is finalised.
  \item \texttt{W522} (Unclosed stream)
\end{itemize}

The list keeps growing: new issues are added as user feedback arrives and as additional edge cases surface during implementation. For example, warning \texttt{W302} was not in the original project proposal but was added later. Once an issue is implemented, its identifier never changes.

% TODO: Update after v1.0.0 has been released.
By release~v0.10.0 (May 2025) the list had grown to 29 warnings and 45 errors~\cite{starynovotny2025warnings}, and more additions are planned for v1.0.0.

\subsection{Confidence-based processing}
\label{sec:confidence-based-processing}
As outlined in Section~\ref{sec:issues}, the linter must handle unexpected input while reporting warnings and errors only when it has high confidence, even if that means missing some issues.

For example, on line 7 of the document in Figure~\ref{fig:iceberg}, two variants \cs{\_\_example\_foo:V} and \texttt{:x} are generated. The former variant is unused, which triggers a warning \texttt{W402}, as expected.

Now assume that instead of the variant argument specifiers \texttt{\{ V, x \}}, line 7 contained input the linter cannot interpret, such as a control sequence: \texttt{\{ \cs{unexpected} \}}. There are two obvious ways in which the linter might react:
\begin{enumerate}
\item[1.] Assume that no variants are generated.
\item[2.] Assume that all variants are generated.
\end{enumerate}

% TODO: Verify the identifier E408 once semantic analysis is finalised.
However, we cannot be confident about either assumption, and both yield incorrect results: In the first approach, an error \texttt{E408} (Calling an undefined function) is reported on line 11, even though the variant \cs{\_\_example\_foo:x} might exist. In the second approach, a warning \texttt{W402} is reported for all other variants, even though they might not exist.

To satisfy our requirements, we used a different approach based on \emph{confidence}:
\begin{enumerate}
\item[3.] Assume that all variants \emph{may be} generated.
\end{enumerate}

With this approach, the linter always adopts the most charitable interpretation and reports only those issues for which it has high confidence.
% TODO: Check that the issues are correctly reported by explcheck v1.0.0.
In our example, this approach reports neither \texttt{E408} nor \texttt{W402}, as expected.

% TODO: Replace the following (slightly redundant) paragraph with different ways in which the concept of confidence is used in explcheck.
% For example, our stepwise approach means that we have token- or call-level information available even in places, where we could not detect any statements. For example, if the control sequence token "\__example_foo:V" appeared at some unusual place, such as on line 12 as "\begin{docu \__example_foo:V ment}", we would use this information to determine that the variant "\__example_foo:V" \emph{may} be used, even though semantic analysis has found no calls to it, and the warning \texttt{W402} would not be reported.
% This makes our approach more flexible and robust than using an end-to-end formal grammar, as discussed with Mathias Jakobsen during a coffee break at TUG 2025 (also reference the TUG article that covers their talk titled "Best-effort TeX parsing for interactive editing").
This confidence-based approach is central to the linter's design. Every statement produced by semantic analysis is tagged with a \emph{confidence level}: \texttt{NONE} (0\%), \texttt{MAYBE} (50\%), or \texttt{DEFINITELY} (100\%). Later, in Section~\ref{sec:code-coverage}, we also use these confidence levels to validate the linter.

\subsection{Limitations}
\label{sec:limitations}

To meet the twelve-month schedule, we restricted the linter to modeling only \TeX's input processor: it tokenizes the source text (see Figure~\ref{fig:iceberg}b) but neither expands macros nor simulates typesetting.

Analysis is statement-oriented (see Figure~\ref{fig:iceberg}d). Other material such as \cs{begin}\texttt{\{document\}} on line~12 serves only to delimit chunks Ch3 and Ch4. Literals and expressions within statements\Dash such as the specifier list \texttt{\{ V, x \}} on line 7, comma lists, boolean expressions, or key--values\Dash are handled ad-hoc or ignored; they have no independent representation.

Removing these constraints would require a major redesign. Section~\ref{sec:future-work} discusses less fundamental limitations that we expect to address in future work.

\section{Implementation}
\label{sec:implementation}

In this section, we present a high-level overview of the implementation. Sections~\ref{sec:package-structure} and \ref{sec:flow-of-information} describe the Lua modules of explcheck and how they interact. Section~\ref{sec:data-model} defines the analysis data model, outlining its entities and relationships.

\subsection{Package structure}
\label{sec:package-structure}

\begin{figure*}
\setlength{\belowcaptionskip}{0.75\baselineskip}
\centering
\includegraphics[scale=0.8]{images/package-diagram}%
\vspace{0.5\baselineskip}%
\caption{Package diagram of explcheck, illustrating the structure and organization of its Lua modules.}
\label{fig:package-diagram}
\end{figure*}

Figure~\ref{fig:package-diagram} illustrates how explcheck's implementation is organized into the following packages (listed from top to bottom):

\begin{itemize}
\item \texttt{analysis}: Performs the processing steps outlined in Section~\ref{sec:processing-steps}.
\item \texttt{evaluation}: Computes statistics from analysis results for verbose command-line output.
\item \texttt{config}: Provides access to configuration, comprising command-line options and config files.
\item \texttt{format}: Formats the command-line output.
\item \texttt{issues}: Maintains a registry of all issues detected during analysis.
\item \texttt{cli}: Implements the command-line interface.
\item \texttt{misc}: Supplies low-level data types and helper functions used by other packages.
\end{itemize}

Since the hierarchical \TeX{} directory structure is generally hidden from users, who load macro packages, fonts, and Lua modules by name, all Lua modules for explcheck reside in a single directory.

The logical package hierarchy is preserved via file naming: a package named \meta{package name} is implemented in the Lua module \texttt{explcheck-\meta{package name}.lua}.\footnote{The only exception is the \texttt{analysis} package, whose Lua modules are named \texttt{explcheck-preprocessing.lua} and \texttt{explcheck-\meta{type}-analysis.lua}, with \meta{type} being \texttt{lexical}, \texttt{syntactic}, \texttt{semantic}, or \texttt{flow}.} For instance, \texttt{misc.latex3} becomes \texttt{explcheck-latex3.lua}.

\subsection{Component interactions}
\label{sec:flow-of-information}

\begin{figure*}
\setlength{\belowcaptionskip}{0.25\baselineskip}
\centering
\begin{tikzpicture}[inner sep=0pt, remember picture]
\node(component-diagram) {\includegraphics[scale=0.8]{images/component-diagram}};
\end{tikzpicture}%
\vspace{0.25\baselineskip}%
\caption{Component diagram of explcheck, showing the interactions between its Lua modules and users.}
\label{fig:component-diagram}
\end{figure*}

Figure~\ref{fig:component-diagram} shows the interactions among explcheck's Lua modules, as well as its inputs and outputs.

Starting from the top-left corner, files loaded from the filesystem are passed through processing steps provided by the \texttt{analysis} package. These steps use the issue registry from the \texttt{issues} package to record warnings and errors. Both the \texttt{analysis} and \texttt{issues} packages rely on configuration settings managed by the \texttt{config} package, which aggregates options from config files and command-line arguments.
\looseness=-1

After analysis completes, its results are evaluated by the \texttt{evaluation} package. Finally, the warnings, errors, and evaluation results are gathered by the central \texttt{cli} package and formatted by the \texttt{format} package to produce either human- or machine-readable output.

\subsection{Data model}
\label{sec:data-model}

\begin{figure*}
\centering
\begin{tikzpicture}[inner sep=0pt, remember picture]
\node(er-diagram) {\includegraphics[scale=0.8]{images/er-diagram}};
\end{tikzpicture}%
\vspace{0.5\baselineskip}%
\caption{Entity relationship diagram of the analysis data model, highlighting our representation of input files.}
\label{fig:er-diagram}
\input images/trace-arrow
\end{figure*}

Figure~\ref{fig:er-diagram} illustrates the internal representation of expl3 input files, constructed incrementally by the processing steps. It expands on the basic data model introduced in Section~\ref{sec:processing-steps}.

Starting from the top-left corner, the input file consists of bytes, which are segmented into expl3 parts during preprocessing. Lexical analysis then converts byte sequences into \TeX{} tokens and identifies brace pairs (\texttt{\{}, \texttt{\}}) as potential \TeX{} groupings.

Syntactic analysis uses these tokens and groupings to recognize function calls and their arguments. Semantic analysis classifies function calls as statements; for function definitions, it identifies replacement text arguments and backtracks to syntactic analysis to analyze nested calls and statements.

Finally, flow analysis merges statements from different segments (such as parts and replacement texts) into chunks of well-understood code and organizes these chunks into a flow graph by connecting them with edges.

\subsection{Inter-file dependencies}
% See:
% - https://github.com/Witiko/expltools/issues/129
% - https://github.com/Witiko/expltools/pull/131

\section{Demonstration}
\label{sec:demonstration}
\subsection{Interfaces}
% Corresponds to parts of Section 4.1 of starynovotny-expltools-slides.pdf.
\subsubsection{Command-line interfaces}
\subsubsection{Lua interface}
\subsection{Integration with text editors}
\subsubsection{Quickfix and M-x compile}
% See:
% - Pages 31 and 32 of starynovotny-expltools-slides.pdf
% - <https://github.com/Witiko/expltools/issues/8#issuecomment-2538185841>
\subsubsection{Language server protocol}
% See <https://github.com/Witiko/expltools/issues/68>.
\subsection{Online resources}
\subsubsection{Docker image}
\subsubsection{List of issues}
% See <https://github.com/Witiko/expltools/issues/28> and <https://github.com/koppor/explcheck-issues>.
\subsubsection{Library of symbols}
% See <https://github.com/Witiko/expltools/issues/33>.
\section{Validation}
\label{sec:validation}
\subsection{Continuous integration}
\subsubsection{Unit tests}
\subsubsection{Regression tests}
\subsection{Exploratory data analysis}
% A back-reference to Section "List of issues".
% See e-mails to devfund@tug.org during April '05.
\subsection{Experiments}
\subsubsection{Prevalence of expl3 in \TeX{} code}
% Corresponds to Section 4.2 of starynovotny-expltools-slides.pdf.
% TODO: Since this has already been "published" in starynovotny-expltools-slides.pdf and https://witiko.github.io/Expl3-Linter-5/, perhaps we can just mention the result as a motivation for caring about expl3 in the last paragraph of Section 1.4 (How expl3 tames the chaos).
\subsubsection{Code coverage}
\label{sec:code-coverage}
\subsubsection{Most common issues}
\subsubsection{Processing speed}
\subsection{Comparison to large language models}
\section{Future work}
\label{sec:future-work}
% TODO: Mention the non-fundamental limitations from [@starynovotny2024project, Section 4.3] that are still relevant in v1.0.0.
% Mention issues from <https://github.com/Witiko/expltools/issues?q=is%3Aissue%20state%3Aopen%20milestone%3A%22explcheck%20v1.1.0%20(after%20first%20stable%20release)%22>.
% (Optional) Mention "TODO"s in the code; this may need adding a separate section after Section "Design" that would discuss the implementation details, so that the content of these "TODO"s is intelligible to the reader.
% Reference [@starynovotny2025warnings, Section "Caveats"].
% Mention the shortcomings of expl3:
% - The ability do delete messages seems unnecessary and makes reasoning about the code more difficult.
% - The lack of a first-class assertion statement.
\section{Related work}
\label{sec:related-work}
% Corresponds to Section 3 of project-proposal.pdf.
% Mention <https://github.com/Hannah-Sten/TeXiFy-IDEA>, as discussed with Erik Nijenhuis at TUG 2025.
% Mention the LaTeX grammar from Mathias Jakobsen's article that covers their TUG 2025 talk titled "Best-effort TeX parsing for interactive editing".
\section{Conclusion}
\label{sec:conclusion}
\section*{Acknowledgements}

\SetBibJustification{\raggedright \advance\itemsep by 2pt plus1pt minus1pt}
\bibliographystyle{tugboat}
\begingroup
\small
\gappto{\UrlBreaks}{\UrlOrds}
\bibliography{tb143starynovotny-expltools}
\endgroup

\makesignature
\end{document}
